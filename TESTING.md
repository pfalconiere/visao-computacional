# ğŸ§ª EstratÃ©gia de Testes - Document Classifier

DocumentaÃ§Ã£o completa da estratÃ©gia de testes para o Document Classifier.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Tipos de Testes](#tipos-de-testes)
- [Estrutura de Testes](#estrutura-de-testes)
- [Como Rodar](#como-rodar)
- [CI/CD Pipeline](#cicd-pipeline)
- [MÃ©tricas e Coverage](#mÃ©tricas-e-coverage)
- [Boas PrÃ¡ticas](#boas-prÃ¡ticas)

---

## ğŸ“Š VisÃ£o Geral

O projeto implementa uma suite completa de testes em **3 nÃ­veis**:

1. **Testes UnitÃ¡rios** (pytest) - Happy + Negative paths
2. **Testes de IntegraÃ§Ã£o API** (Postman + Newman) - Happy + Negative paths  
3. **Testes de CI/CD** (GitHub Actions) - AutomaÃ§Ã£o completa

### **Filosofia de Testes:**

âœ… **Test Early, Test Often**  
- Testes rodam ANTES de cada commit (pre-commit hooks)  
- Testes rodam AUTOMATICAMENTE em cada push (CI/CD)  
- Deploy BLOQUEADO se testes falharem  

âœ… **Happy Path + Negative Path**  
- Cada funcionalidade tem teste de sucesso (happy path)  
- Cada funcionalidade tem teste de erro (negative path)  

âœ… **Coverage > 70%**  
- Meta: 70%+ de code coverage  
- RelatÃ³rios automÃ¡ticos em cada execuÃ§Ã£o  

---

## ğŸ§ª Tipos de Testes

### **1ï¸âƒ£ Testes UnitÃ¡rios (Pytest)**

**LocalizaÃ§Ã£o:** `tests/`  
**Framework:** pytest + pytest-cov  
**Executar:** `./run_tests.sh unit`  

#### **MÃ³dulos Testados:**

| MÃ³dulo | Arquivo | Happy Paths | Negative Paths | Coverage Meta |
|--------|---------|-------------|----------------|---------------|
| `ClassificadorFinal` | `test_classificador.py` | 4 | 6 | 80% |
| `API Flask` | `test_api.py` | 13 | 7 | 70% |
| `TextAnalyzer` | `test_text_analyzer.py` | 5 | 3 | 60% |
| `ParagraphDetector` | `test_paragraph_detector.py` | 4 | 3 | 60% |

#### **Exemplos de Testes:**

**Happy Path:**
```python
def test_classify_advertisement_happy_path(mock_image):
    """
    Input: Imagem de advertisement
    Expected: classification='advertisement', success=True
    """
    result = clf.classify(mock_image)
    assert result['success'] is True
    assert result['classification'] == 'advertisement'
```

**Negative Path:**
```python
def test_classify_nonexistent_file_negative():
    """
    Input: Arquivo inexistente
    Expected: ExceÃ§Ã£o levantada
    """
    with pytest.raises(Exception):
        clf.classify('/fake/path.tif')
```

### **2ï¸âƒ£ Testes de API (Postman + Newman)**

**LocalizaÃ§Ã£o:** `postman/`  
**Framework:** Postman Collection + Newman CLI  
**Executar:** `./run_tests.sh api`  

#### **Endpoints Testados:**

| Endpoint | Happy Paths | Negative Paths | Total |
|----------|-------------|----------------|-------|
| Health Checks | 3 | 1 | 4 |
| Classification | 2 | 4 | 6 |
| Async | 2 | 0 | 2 |
| Feedback | 3 | 2 | 5 |
| **TOTAL** | **10** | **7** | **17** |

#### **Exemplo de Teste:**

```javascript
pm.test("Classification returns valid result", function () {
    var jsonData = pm.response.json();
    pm.expect(jsonData).to.have.property('classification');
    pm.expect(jsonData.classification).to.be.oneOf(['advertisement', 'scientific_article']);
    pm.expect(jsonData.success).to.eql(true);
});
```

### **3ï¸âƒ£ Testes de CI/CD (GitHub Actions)**

**LocalizaÃ§Ã£o:** `.github/workflows/ci.yml`  
**Trigger:** Push para `main`, PRs  
**Executar:** AutomÃ¡tico no GitHub  

#### **Pipeline:**

```mermaid
graph LR
    A[Push/PR] --> B[Testes UnitÃ¡rios]
    B -->|âœ… Pass| C[Build & Health Check]
    C -->|âœ… Pass| D[Testes de API]
    D -->|âœ… Pass| E[Deploy]
    E -->|âœ… Success| F[VerificaÃ§Ã£o Prod]
    
    B -->|âŒ Fail| G[Block Deploy]
    C -->|âŒ Fail| G
    D -->|âŒ Fail| G
```

---

## ğŸ“ Estrutura de Testes

```
document_classifier_project/
â”œâ”€â”€ tests/                              # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_classificador.py           # Testes do ClassificadorFinal
â”‚   â”œâ”€â”€ test_api.py                     # Testes da API Flask
â”‚   â”œâ”€â”€ test_text_analyzer.py           # Testes do TextAnalyzer
â”‚   â”œâ”€â”€ test_paragraph_detector.py      # Testes do ParagraphDetector
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ postman/                            # Testes de API
â”‚   â”œâ”€â”€ document-classifier-api.postman_collection.json
â”‚   â”œâ”€â”€ document-classifier.postman_environment.json
â”‚   â”œâ”€â”€ document-classifier-production.postman_environment.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ test_images/                        # Imagens para testes
â”‚   â”œâ”€â”€ advertisement.tif               # (adicionar manualmente)
â”‚   â”œâ”€â”€ scientific.tif                  # (adicionar manualmente)
â”‚   â”œâ”€â”€ invalid.jpg                     # (adicionar manualmente)
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .github/workflows/                  # CI/CD
â”‚   â””â”€â”€ ci.yml                          # GitHub Actions workflow
â”‚
â”œâ”€â”€ pytest.ini                          # ConfiguraÃ§Ã£o do pytest
â”œâ”€â”€ .pre-commit-config.yaml             # Pre-commit hooks
â”œâ”€â”€ requirements-dev.txt                # DependÃªncias de teste
â”œâ”€â”€ run_tests.sh                        # Script para rodar testes
â””â”€â”€ TESTING.md                          # Este arquivo
```

---

## ğŸš€ Como Rodar

### **InstalaÃ§Ã£o:**

```bash
# DependÃªncias principais
pip install -r requirements.txt

# DependÃªncias de teste
pip install -r requirements-dev.txt

# Newman (para testes de API)
npm install -g newman newman-reporter-htmlextra

# Pre-commit hooks (opcional)
pip install pre-commit
pre-commit install
```

### **Rodar Testes:**

#### **OpÃ§Ã£o 1: Script Facilitado**

```bash
# Todos os testes
./run_tests.sh all

# Apenas testes unitÃ¡rios
./run_tests.sh unit

# Apenas testes de API
./run_tests.sh api

# Apenas happy paths
./run_tests.sh happy

# Apenas negative paths
./run_tests.sh negative

# Gerar relatÃ³rio de coverage
./run_tests.sh coverage

# Testes rÃ¡pidos
./run_tests.sh quick

# Limpar arquivos de teste
./run_tests.sh clean
```

#### **OpÃ§Ã£o 2: Comandos Diretos**

**Pytest:**
```bash
# Todos os testes
pytest tests/ -v

# Com coverage
pytest tests/ --cov=. --cov-report=html

# Apenas um arquivo
pytest tests/test_api.py -v

# Apenas um teste
pytest tests/test_api.py::TestHealthEndpoints::test_health_endpoint_happy_path -v
```

**Newman:**
```bash
# Local
newman run postman/document-classifier-api.postman_collection.json \
  -e postman/document-classifier.postman_environment.json \
  --reporters cli,htmlextra \
  --reporter-htmlextra-export newman/report.html

# ProduÃ§Ã£o
newman run postman/document-classifier-api.postman_collection.json \
  -e postman/document-classifier-production.postman_environment.json
```

---

## ğŸ”„ CI/CD Pipeline

### **Fluxo Completo:**

1. **Developer faz commit localmente**
   - Pre-commit hook roda testes unitÃ¡rios
   - Se falhar â†’ Commit bloqueado
   - Se passar â†’ Commit permitido

2. **Developer faz push para GitHub**
   - GitHub Actions disparado automaticamente
   - **Job 1:** Testes UnitÃ¡rios (pytest)
     - Roda todos os testes em `tests/`
     - Gera coverage report
     - Falha â†’ Pipeline PARA aqui
   
3. **Se Job 1 passar:**
   - **Job 2:** Build & Health Check
     - Verifica imports
     - Inicia API
     - Testa endpoints crÃ­ticos
     - Falha â†’ Pipeline PARA aqui
   
4. **Se Job 2 passar:**
   - **Job 3:** Testes de API (Newman)
     - Roda collection completa
     - 17 testes de integraÃ§Ã£o
     - Gera relatÃ³rio HTML
     - Falha â†’ Pipeline PARA aqui
   
5. **Se Job 3 passar (apenas em push para `main`):**
   - **Job 4:** Deploy para Render
     - Dispara webhook de deploy
     - Aguarda deploy completar
     - Verifica health check em produÃ§Ã£o
   
6. **Se Job 4 passar:**
   - **Job 5:** NotificaÃ§Ã£o de sucesso
     - Log de sucesso
     - URLs da aplicaÃ§Ã£o

### **Tempo Estimado:**

- Testes UnitÃ¡rios: ~30s
- Build & Health Check: ~45s
- Testes de API: ~1-2min
- Deploy: ~2-3min
- **Total: ~5-6 minutos**

### **Segredos NecessÃ¡rios no GitHub:**

```bash
# .github/secrets
RENDER_DEPLOY_HOOK=https://api.render.com/deploy/...
```

---

## ğŸ“Š MÃ©tricas e Coverage

### **RelatÃ³rios Gerados:**

| Ferramenta | RelatÃ³rio | LocalizaÃ§Ã£o |
|------------|-----------|-------------|
| Pytest | HTML Coverage | `htmlcov/index.html` |
| Pytest | XML Coverage | `coverage.xml` |
| Pytest | JUnit XML | `junit/test-results.xml` |
| Newman | HTML Report | `newman/report.html` |
| Newman | JUnit XML | `newman/junit-report.xml` |

### **Visualizar RelatÃ³rios:**

```bash
# Coverage local
./run_tests.sh coverage
# Abre automaticamente: htmlcov/index.html

# Newman local
./run_tests.sh api
# Abre automaticamente: newman/report.html

# GitHub Actions
# Ver: Actions tab â†’ Workflow run â†’ Artifacts
```

### **Metas de Coverage:**

| MÃ³dulo | Meta | Atual |
|--------|------|-------|
| `api.py` | 70% | - |
| `classificador_final.py` | 80% | - |
| `text_analyzer*.py` | 60% | - |
| `paragraph_detector.py` | 60% | - |
| **Projeto Total** | **70%** | - |

---

## âœ… Boas PrÃ¡ticas

### **1. Nomenclatura de Testes:**

```python
def test_<funcao>_<cenario>_<tipo>():
    """
    HAPPY/NEGATIVE PATH: DescriÃ§Ã£o clara
    
    Input: O que entra
    Expected: O que deve sair
    """
    # Arrange (preparar)
    # Act (executar)
    # Assert (verificar)
```

### **2. Estrutura AAA (Arrange-Act-Assert):**

```python
def test_classify_advertisement():
    # Arrange: Preparar dados
    image_path = create_mock_advertisement()
    
    # Act: Executar funÃ§Ã£o
    result = classifier.classify(image_path)
    
    # Assert: Verificar resultado
    assert result['classification'] == 'advertisement'
```

### **3. Fixtures para ReutilizaÃ§Ã£o:**

```python
@pytest.fixture
def mock_image():
    """Criar imagem de teste (reutilizÃ¡vel)"""
    img = Image.new('RGB', (800, 600), color='white')
    # ... configurar imagem
    yield img
    # ... cleanup se necessÃ¡rio
```

### **4. Testar Edge Cases:**

```python
# Happy Path
def test_with_valid_input():
    pass

# Negative Paths
def test_with_empty_input():
    pass

def test_with_invalid_format():
    pass

def test_with_corrupted_data():
    pass

def test_with_extreme_values():
    pass
```

### **5. Mocks para DependÃªncias Externas:**

```python
from unittest.mock import Mock, patch

@patch('api.ClassificadorFinal')
def test_api_endpoint(mock_classifier):
    mock_classifier.return_value.classify.return_value = {'success': True}
    # Testar endpoint sem depender do classificador real
```

### **6. Timeouts para Testes Lentos:**

```python
@pytest.mark.timeout(30)
def test_slow_ocr_process():
    # Falha se demorar > 30s
    result = process_ocr(large_image)
```

### **7. Skip Condicional:**

```python
@pytest.mark.skipif(not has_tesseract(), reason="Tesseract nÃ£o instalado")
def test_ocr_extraction():
    pass
```

---

## ğŸ› Debugging e Troubleshooting

### **Testes UnitÃ¡rios Falhando:**

```bash
# Modo verboso
pytest tests/ -vv -s

# Debugger
pytest tests/test_api.py --pdb

# Apenas testes que falharam
pytest tests/ --lf
```

### **Newman Falhando:**

```bash
# Verificar se API estÃ¡ rodando
curl http://localhost:5000/health

# Aumentar timeout
newman run ... --timeout-request 120000

# Ver logs detalhados
newman run ... --verbose
```

### **GitHub Actions Falhando:**

1. Ver logs: `Actions tab â†’ Workflow run â†’ Job que falhou`
2. Reproduzir localmente: `./run_tests.sh all`
3. Verificar dependÃªncias: `pip list`
4. Verificar Python version: `python --version`

---

## ğŸ“š Recursos Adicionais

- **Pytest Docs:** https://docs.pytest.org/
- **Newman Docs:** https://learning.postman.com/docs/running-collections/using-newman-cli/command-line-integration-with-newman/
- **GitHub Actions:** https://docs.github.com/en/actions
- **Pre-commit:** https://pre-commit.com/

---

## ğŸ¯ Checklist de Testes

Antes de fazer commit:

- [ ] Rodar testes unitÃ¡rios: `./run_tests.sh unit`
- [ ] Coverage > 70%: `./run_tests.sh coverage`
- [ ] Testes de API passam (se API rodando): `./run_tests.sh api`
- [ ] CÃ³digo formatado: `black .`
- [ ] Imports ordenados: `isort .`
- [ ] Lint pass: `flake8 .`

Antes de fazer deploy:

- [ ] Todos os testes passam: `./run_tests.sh all`
- [ ] Pre-commit hooks configurados: `pre-commit install`
- [ ] GitHub Actions verde âœ…
- [ ] Imagens de teste adicionadas (se necessÃ¡rio)
- [ ] README atualizado

---

**âœ… Com essa suite de testes, garantimos qualidade, confiabilidade e entrega contÃ­nua!** ğŸš€

